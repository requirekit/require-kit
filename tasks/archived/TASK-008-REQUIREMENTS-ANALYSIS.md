# TASK-008 Requirements Analysis for Python Implementation

**Task**: Review and Enhance /feature-generate-tasks for Complexity Control
**Priority**: High
**Status**: In Progress
**Analysis Date**: 2025-10-11

---

## Executive Summary

TASK-008 requires enhancing the `/feature-generate-tasks` command to integrate with complexity evaluation logic from TASK-005 and TASK-006. The goal is to ensure all generated tasks are appropriately scoped (complexity ‚â§ 7) before being created, preventing the creation of overly complex tasks that would fail architectural review or require excessive implementation time.

---

## 1. Functional Requirements

### FR-1: Integration with Complexity Evaluation Module
**Source**: KEY REQUIREMENT #1
**Priority**: CRITICAL

**EARS Statement**: When `/feature-generate-tasks` generates task proposals, the system shall evaluate each task's complexity using the same complexity evaluation module used by `/task-create`.

**Implementation Requirements**:
- Import and use existing complexity evaluation module (from TASK-005)
- Apply complexity scoring to each proposed task before creation
- Use consistent scoring algorithm (1-10 scale) across `/task-create` and `/feature-generate-tasks`
- Extract complexity factors from task proposal content:
  - File count (files to create/modify)
  - Design patterns mentioned
  - External dependencies
  - Risk indicators (security, schema changes, performance)

**Integration Points**:
```python
# Expected module structure (from TASK-005/006)
from installer.global.commands.lib.complexity_evaluator import ComplexityEvaluator

evaluator = ComplexityEvaluator()
complexity_score = evaluator.evaluate(task_proposal)
# Returns: ComplexityScore object with:
#   - total_score (1-10)
#   - level ('simple', 'medium', 'complex')
#   - factors (file_complexity, pattern_familiarity, risk_level)
```

**Acceptance Criteria**:
- All proposed tasks must be evaluated for complexity
- Complexity scores must match `/task-create` scoring for identical task descriptions
- Evaluation must occur before task file creation

**Questions/Gaps**:
- Does the complexity evaluation module accept different input formats (feature description vs task proposal)?
- Are there feature-specific complexity factors to consider (e.g., BDD scenario count)?
- Should we cache complexity evaluations for identical task descriptions?

---

### FR-2: Automatic Task Breakdown for Complex Tasks
**Source**: KEY REQUIREMENT #2
**Priority**: CRITICAL

**EARS Statement**: When a proposed task receives a complexity score ‚â• 7, the system shall automatically break it down into smaller sub-tasks with complexity scores ‚â§ 6.

**Implementation Requirements**:
- Implement task breakdown algorithm that:
  - Identifies logical decomposition boundaries
  - Maintains parent-child task relationships
  - Preserves traceability to original requirements
  - Ensures sub-tasks collectively fulfill parent task requirements

**Breakdown Strategies** (from feature-generate-tasks.md patterns):
1. **By Task Type**: UI ‚Üí API ‚Üí Testing ‚Üí Documentation
2. **By EARS Requirements**: One task per requirement
3. **By BDD Scenarios**: One task per scenario or Given/When/Then clause
4. **By Acceptance Criteria**: One task per criterion
5. **By File Changes**: Group related file modifications

**Example Breakdown Logic**:
```python
# High-level algorithm
def break_down_complex_task(task_proposal, complexity_score):
    """
    Breaks down a complex task (score >= 7) into simpler sub-tasks.

    Args:
        task_proposal: Original task with title, description, requirements
        complexity_score: ComplexityScore object indicating why task is complex

    Returns:
        List of sub-task proposals, each with complexity <= 6
    """
    if complexity_score.factors.file_complexity >= 3:
        # High file count - break down by file or module
        sub_tasks = break_down_by_files(task_proposal)

    elif complexity_score.factors.risk_level >= 3:
        # High risk - separate planning, implementation, validation
        sub_tasks = break_down_by_risk_mitigation(task_proposal)

    elif len(task_proposal.requirements) > 3:
        # Multiple requirements - one task per requirement
        sub_tasks = break_down_by_requirements(task_proposal)

    else:
        # Default - break by implementation phases
        sub_tasks = break_down_by_phases(task_proposal)

    # Re-evaluate sub-tasks to ensure complexity <= 6
    for sub_task in sub_tasks:
        sub_complexity = evaluate_complexity(sub_task)
        if sub_complexity.total_score >= 7:
            # Recursively break down further
            sub_tasks.extend(break_down_complex_task(sub_task, sub_complexity))

    return sub_tasks
```

**Acceptance Criteria**:
- No tasks with complexity > 7 are created without breakdown
- Sub-tasks collectively cover all parent task requirements
- Sub-task complexity scores are ‚â§ 6
- Parent-child relationships are documented in task metadata
- Original complex task proposal is saved as documentation (not as executable task)

**Questions/Gaps**:
- What is the maximum recursion depth for breakdown?
- Should we prompt user to confirm breakdown strategy?
- How do we handle tasks that are inherently indivisible (e.g., "Deploy to production")?
- Do we need a minimum task size to prevent over-decomposition?

---

### FR-3: Complexity Visualization in Generation Output
**Source**: KEY REQUIREMENT #3
**Priority**: HIGH

**EARS Statement**: When displaying generated task proposals, the system shall show complexity scores with visual indicators for each task.

**Implementation Requirements**:
- Display complexity score (1-10) and level (simple/medium/complex)
- Use visual indicators:
  - üü¢ Green: Complexity 1-3 (Simple)
  - üü° Yellow: Complexity 4-6 (Medium)
  - üî¥ Red: Complexity 7-10 (Complex - will be broken down)
- Show complexity distribution statistics:
  - Average complexity across all tasks
  - Count of tasks by complexity level
  - Highest complexity task

**Output Format Example**:
```
üìã Generated Tasks (10)

üé® UI/UX Tasks
‚úÖ TASK-043: Design authentication UI components
   Complexity: 4/10 üü° Medium | Timeline: 2 days
   Factors: 3 files, 1 pattern, low risk

‚úÖ TASK-044: Implement responsive login form
   Complexity: 3/10 üü¢ Simple | Timeline: 1 day
   Factors: 2 files, standard patterns, low risk

üîå API/Backend Tasks
‚úÖ TASK-045: Implement authentication API endpoint
   Complexity: 6/10 üü° Medium | Timeline: 2 days
   Factors: 4 files, 2 patterns, medium risk

‚úÖ TASK-046: Add session management system
   Complexity: 7/10 üî¥ Complex ‚Üí Auto-breaking down...

   Breakdown into 3 sub-tasks:
   ‚úÖ TASK-046.1: Design session storage architecture
      Complexity: 4/10 üü° Medium | Timeline: 1 day

   ‚úÖ TASK-046.2: Implement session CRUD operations
      Complexity: 5/10 üü° Medium | Timeline: 1 day

   ‚úÖ TASK-046.3: Add session security and expiration
      Complexity: 5/10 üü° Medium | Timeline: 1 day

üìä Complexity Distribution
Average: 4.2/10 (Medium)
Simple (1-3): 3 tasks (30%)
Medium (4-6): 6 tasks (60%)
Complex (7-10): 1 task ‚Üí Broken down into 3 sub-tasks
```

**Acceptance Criteria**:
- Complexity score and visual indicator displayed for every task
- Color-coded indicators match complexity level
- Average complexity and distribution statistics shown
- Breakdown decisions are clearly explained

**Questions/Gaps**:
- Should we show complexity factors for each task or only on demand?
- Do we need to support terminal environments that don't render emoji?
- Should we log complexity data to state files for historical analysis?

---

### FR-4: Interactive Complexity Adjustment
**Source**: KEY REQUIREMENT #4
**Priority**: MEDIUM

**EARS Statement**: While in `--interactive` mode, the system shall allow users to customize the complexity threshold for automatic breakdown.

**Implementation Requirements**:
- Add complexity threshold prompt in interactive mode
- Allow user to set custom threshold (default: 7)
- Allow user to override automatic breakdown decisions
- Provide real-time re-evaluation with custom threshold

**Interactive Flow**:
```
üîÑ Interactive Task Generation: FEAT-001

‚öôÔ∏è  Complexity Settings
Current threshold: 7/10 (auto-breakdown for scores >= 7)

Adjust threshold? [y/N]: y
Enter new threshold (1-10, recommended 6-8): 6

‚úÖ Threshold updated to 6/10

üìã Task Complexity Review
Task 5: Add session management system
Current complexity: 7/10 üî¥ Complex
Recommendation: Break down into sub-tasks

Options:
[A] Auto-breakdown (recommended) - Creates 3 sub-tasks
[M] Manual adjustment - Edit task scope to reduce complexity
[K] Keep as-is - Override threshold (not recommended)
[V] View complexity factors - See detailed breakdown

Your choice (A/M/K/V):
```

**Acceptance Criteria**:
- Interactive mode allows threshold customization
- Custom threshold applies to all tasks in the feature
- Users can override breakdown decisions with clear warnings
- Complexity factors can be viewed on demand

**Questions/Gaps**:
- Should custom thresholds be saved to feature metadata?
- Can users provide breakdown hints (e.g., "split by file" vs "split by requirement")?
- How do we handle user overrides that create high-complexity tasks?

---

### FR-5: Quality Gate Integration
**Source**: KEY REQUIREMENT #5
**Priority**: HIGH

**EARS Statement**: When generating tasks, the system shall ensure generated tasks align with quality gate requirements from `/task-work`.

**Implementation Requirements**:
- Validate that each task has testable acceptance criteria
- Ensure tasks specify required test types (unit/integration/e2e)
- Auto-generate testing tasks for high-complexity features
- Link tasks to requirements and BDD scenarios

**Quality Gate Alignment**:
| Quality Gate (from task-work.md) | Task Generation Requirement |
|----------------------------------|----------------------------|
| Code compiles (100%) | Tasks must specify technology stack |
| All tests pass (100%) | Tasks must include test expectations |
| Line coverage ‚â• 80% | Testing tasks auto-generated |
| Branch coverage ‚â• 75% | Edge cases identified in task description |

**Testing Task Generation Rules**:
```python
# Auto-generate testing tasks for high-complexity features
if feature_has_high_complexity_tasks or total_task_count > 5:
    testing_tasks = [
        create_unit_test_task(implementation_tasks),
        create_integration_test_task(api_tasks),
        create_e2e_test_task(user_flows)
    ]
    generated_tasks.extend(testing_tasks)
```

**Acceptance Criteria**:
- All tasks have at least one testable acceptance criterion
- Testing tasks are automatically generated for features with 5+ implementation tasks
- Tasks reference specific requirements (REQ-XXX) and BDD scenarios (BDD-XXX)
- Task metadata includes expected test types

**Questions/Gaps**:
- Should we validate that acceptance criteria are SMART (Specific, Measurable, Achievable, Relevant, Time-bound)?
- Can we detect missing edge cases from EARS requirements?
- Should testing tasks have a fixed complexity score or be evaluated like other tasks?

---

### FR-6: Duplicate Task Detection
**Source**: KEY REQUIREMENT #6 (from existing feature-generate-tasks.md)
**Priority**: HIGH

**EARS Statement**: When generating tasks, the system shall validate that no duplicate tasks exist across all task directories.

**Implementation Requirements**:
- Search all task directories: backlog, in_progress, blocked, in_review, completed
- Compare task titles using fuzzy matching (handle typos, slight variations)
- Check for duplicate feature-task relationships (same feature, similar scope)
- Warn user before creating potential duplicates

**Duplicate Detection Logic**:
```python
def check_for_duplicates(proposed_task, epic_id, feature_id):
    """
    Checks if a similar task already exists.

    Returns:
        DuplicateResult with:
        - is_duplicate (bool)
        - similar_tasks (list of Task objects with similarity scores)
        - recommendation (str: 'skip', 'merge', 'rename', 'create')
    """
    # Search all task directories
    existing_tasks = find_tasks_by_feature(feature_id) + find_tasks_by_epic(epic_id)

    # Fuzzy match on title
    similar_tasks = []
    for task in existing_tasks:
        similarity = calculate_similarity(proposed_task.title, task.title)
        if similarity >= 0.8:  # 80% similar
            similar_tasks.append((task, similarity))

    # Check for scope overlap
    for task in existing_tasks:
        if has_overlapping_acceptance_criteria(proposed_task, task):
            similar_tasks.append((task, 0.9))  # High similarity for scope overlap

    if similar_tasks:
        return DuplicateResult(
            is_duplicate=True,
            similar_tasks=similar_tasks,
            recommendation=recommend_action(similar_tasks)
        )

    return DuplicateResult(is_duplicate=False)
```

**Acceptance Criteria**:
- Duplicate detection runs before any task file is created
- Users are warned of potential duplicates with similarity scores
- Users can view existing task details before deciding
- No duplicate tasks are created without explicit user confirmation

**Questions/Gaps**:
- What similarity threshold should trigger duplicate warning (80%, 85%, 90%)?
- Should we use NLP/semantic similarity or simple string matching?
- Do we need to check for duplicates across different features/epics?

---

## 2. Non-Functional Requirements

### NFR-1: Performance
**Priority**: MEDIUM

**Requirement**: The feature generation command shall complete within 30 seconds for features with up to 20 proposed tasks.

**Constraints**:
- Complexity evaluation per task: < 500ms
- Duplicate detection per task: < 200ms
- Breakdown algorithm per complex task: < 2 seconds
- Total generation time: < 30 seconds for 20 tasks

**Acceptance Criteria**:
- 95th percentile completion time ‚â§ 30 seconds
- No blocking operations during generation
- Progress indicators shown for long-running operations

---

### NFR-2: Usability
**Priority**: HIGH

**Requirement**: The system shall provide clear, actionable feedback at each step of task generation.

**Requirements**:
- Display progress indicators during generation
- Show complexity evaluation results in human-readable format
- Explain why tasks are being broken down
- Provide clear next steps after generation

**Acceptance Criteria**:
- Users can understand complexity scores without technical documentation
- Breakdown decisions include rationale
- Error messages are actionable
- Output follows existing `/feature-generate-tasks` format conventions

---

### NFR-3: Maintainability
**Priority**: HIGH

**Requirement**: The complexity integration shall use the same evaluation logic as `/task-create` to ensure consistency.

**Constraints**:
- No code duplication between `/task-create` and `/feature-generate-tasks`
- Shared complexity evaluation module
- Consistent data structures (ComplexityScore, TaskProposal)
- Unified configuration (complexity thresholds, breakdown strategies)

**Acceptance Criteria**:
- Complexity module is imported, not duplicated
- Configuration changes apply to both commands
- Unit tests verify consistency between commands

---

### NFR-4: Testability
**Priority**: HIGH

**Requirement**: All complexity evaluation and breakdown logic shall be unit tested with ‚â• 80% coverage.

**Test Requirements**:
- Unit tests for complexity evaluation with various task proposals
- Unit tests for breakdown algorithms with edge cases
- Integration tests for end-to-end task generation
- Tests for duplicate detection with similar task titles
- Tests for interactive mode user flows

**Acceptance Criteria**:
- ‚â• 80% line coverage for new code
- ‚â• 75% branch coverage for breakdown logic
- All edge cases documented and tested

---

## 3. Testable Acceptance Criteria

### AC-1: Complexity Evaluation Applied
‚úÖ CRITICAL

**Given** a feature with 5 proposed tasks
**When** `/feature-generate-tasks FEAT-001` is executed
**Then** each task must have a complexity score (1-10)
**And** complexity scores must match `/task-create` for identical task descriptions

**Test Strategy**:
- Create feature with varied task proposals (simple, medium, complex)
- Run `/feature-generate-tasks`
- Verify all tasks have complexity scores
- Compare with `/task-create` scoring for same task descriptions

---

### AC-2: Automatic Breakdown for Complex Tasks
‚úÖ CRITICAL

**Given** a proposed task with complexity ‚â• 7
**When** `/feature-generate-tasks` evaluates the task
**Then** the task must be automatically broken down into sub-tasks
**And** all sub-tasks must have complexity ‚â§ 6
**And** no task with complexity > 7 is created

**Test Strategy**:
- Create task proposal with known high complexity factors (many files, multiple patterns, high risk)
- Run generation
- Verify task is broken down
- Verify all created tasks have complexity ‚â§ 7

---

### AC-3: Complexity Visualization
‚úÖ HIGH

**Given** generated tasks with varying complexity
**When** task list is displayed
**Then** each task must show complexity score and visual indicator
**And** distribution statistics must be shown (average, count by level)

**Test Strategy**:
- Generate tasks
- Parse output for complexity scores and emoji indicators
- Verify statistics are calculated correctly
- Test with terminal environments (emoji support and plain text fallback)

---

### AC-4: Interactive Threshold Customization
‚úÖ MEDIUM

**Given** user runs `/feature-generate-tasks FEAT-001 --interactive`
**When** prompted for complexity threshold
**Then** user can set custom threshold (1-10)
**And** breakdown decisions use custom threshold

**Test Strategy**:
- Run interactive mode with mocked user input
- Set custom threshold (e.g., 6)
- Verify tasks with complexity ‚â• 6 are flagged for breakdown
- Verify user can override breakdown decisions

---

### AC-5: Testing Tasks Auto-Generated
‚úÖ HIGH

**Given** a feature with high-complexity tasks
**When** `/feature-generate-tasks` completes
**Then** testing tasks must be automatically created
**And** testing tasks must cover unit, integration, and e2e tests

**Test Strategy**:
- Create feature with complex implementation tasks
- Verify testing tasks are generated
- Verify testing tasks reference implementation tasks
- Check testing tasks have appropriate acceptance criteria

---

### AC-6: Duplicate Prevention
‚úÖ HIGH

**Given** existing tasks in the system
**When** `/feature-generate-tasks` proposes a similar task
**Then** user must be warned of potential duplicate
**And** no duplicate is created without explicit confirmation

**Test Strategy**:
- Create existing task: "Implement user authentication"
- Generate new task: "Add user authentication system"
- Verify similarity warning is shown
- Verify task is not created without confirmation

---

### AC-7: Documentation Updated
‚úÖ MEDIUM

**Given** complexity control features are implemented
**When** code changes are complete
**Then** `feature-generate-tasks.md` must document complexity controls
**And** examples must show complexity visualization and breakdown

**Test Strategy**:
- Review documentation for:
  - Complexity evaluation description
  - Breakdown algorithm explanation
  - Interactive mode usage
  - Example output with complexity scores
- Verify examples match actual implementation

---

### AC-8: Complexity Metadata in Frontmatter
‚úÖ MEDIUM

**Given** tasks are generated with complexity evaluation
**When** task files are created
**Then** frontmatter must include:
  - `complexity_score` (1-10)
  - `complexity_level` (simple/medium/complex)
  - `complexity_factors` (file count, patterns, risks)

**Test Strategy**:
- Generate tasks
- Parse task file frontmatter
- Verify all complexity fields are present
- Verify values match evaluation results

---

### AC-9: No Complexity > 7 Created
‚úÖ CRITICAL

**Given** any feature being processed
**When** `/feature-generate-tasks` completes
**Then** no task file with `complexity_score > 7` exists in any task directory
**And** complex tasks must have been broken down into sub-tasks

**Test Strategy**:
- Search all task directories after generation
- Parse all task frontmatter
- Assert `complexity_score <= 7` for all tasks
- Verify breakdown log shows complex tasks were decomposed

---

### AC-10: Consistency with /task-create
‚úÖ CRITICAL

**Given** the same task description
**When** evaluated by `/task-create` and `/feature-generate-tasks`
**Then** complexity scores must be identical
**And** breakdown decisions must be identical

**Test Strategy**:
- Create test cases with identical task descriptions
- Run through both commands
- Compare complexity scores
- Compare breakdown decisions
- Assert no drift between implementations

---

## 4. Integration Points

### IP-1: Complexity Evaluation Module (from TASK-005)
**Status**: UNKNOWN (TASK-005 completion status not provided)

**Expected Interface**:
```python
# Module: installer/global/commands/lib/complexity_evaluator.py

class ComplexityEvaluator:
    def evaluate(self, task_description: str, requirements: List[str] = None,
                 metadata: dict = None) -> ComplexityScore:
        """
        Evaluates task complexity on 1-10 scale.

        Args:
            task_description: Task title and description
            requirements: List of REQ-XXX IDs linked to task
            metadata: Additional context (file count, patterns, dependencies)

        Returns:
            ComplexityScore with total_score, level, and factors
        """
        pass

class ComplexityScore:
    total_score: int  # 1-10
    level: str  # 'simple' (1-3), 'medium' (4-6), 'complex' (7-10)
    factors: ComplexityFactors
    rationale: str  # Human-readable explanation

class ComplexityFactors:
    file_complexity: float  # 0-3 points
    pattern_familiarity: float  # 0-2 points
    risk_level: float  # 0-3 points
    dependency_count: float  # 0-2 points
```

**Integration Questions**:
1. Does this module exist at the expected path?
2. Does it accept the expected input formats?
3. Are there stack-specific evaluators (MAUI, React, Python)?
4. How does it handle tasks without detailed metadata?

**Verification**:
- [ ] Locate complexity_evaluator.py
- [ ] Review actual API vs expected API
- [ ] Test with sample task descriptions
- [ ] Document any deviations

---

### IP-2: Task Creation from /task-create (from TASK-006)
**Status**: UNKNOWN (TASK-006 completion status not provided)

**Expected Integration**:
- `/task-create` already uses complexity evaluation module
- `/task-create` already has breakdown logic (or does it?)
- Shared configuration for thresholds and breakdown strategies

**Questions**:
1. Does `/task-create` perform automatic breakdown for complex tasks?
2. If so, can we reuse the breakdown algorithm?
3. Are there differences in how standalone tasks vs feature-generated tasks are evaluated?

**Verification**:
- [ ] Review `/task-create` command implementation
- [ ] Check if breakdown logic exists
- [ ] Identify reusable components
- [ ] Document integration approach

---

### IP-3: Task Metadata Structure
**Current State**: Documented in feature-generate-tasks.md

**Required Extensions**:
```yaml
# Task frontmatter additions
complexity:
  score: 7  # Total complexity score
  level: "complex"  # simple, medium, complex
  factors:
    file_complexity: 2.5
    pattern_familiarity: 1.8
    risk_level: 2.2
    dependency_count: 0.5
  evaluated_at: "2025-10-11T14:30:00Z"
  evaluated_by: "feature-generate-tasks"
  breakdown_parent: "TASK-046"  # If this is a sub-task
  breakdown_children: ["TASK-046.1", "TASK-046.2"]  # If this was broken down
```

**Verification**:
- [ ] Confirm frontmatter schema is extensible
- [ ] Test YAML parsing with new fields
- [ ] Ensure backward compatibility

---

### IP-4: Feature Context
**Current State**: Features link to requirements and BDD scenarios

**Required Data**:
- Feature requirements (REQ-XXX IDs)
- BDD scenarios (BDD-XXX IDs)
- Acceptance criteria
- Epic context
- Technology stack

**Data Flow**:
```
Feature File
  ‚Üì
feature-generate-tasks parses:
  - Requirements
  - BDD scenarios
  - Acceptance criteria
  ‚Üì
For each proposed task:
  - Extract relevant requirements
  - Analyze task scope
  - Evaluate complexity
  - Apply breakdown if needed
  - Create task with metadata
```

**Verification**:
- [ ] Review feature file schema
- [ ] Test parsing with complex features
- [ ] Ensure all context is preserved in generated tasks

---

## 5. Task Breakdown Algorithms

### Algorithm 1: Break Down by Requirements
**When to Use**: Task covers multiple independent requirements

**Logic**:
```python
def break_down_by_requirements(task_proposal):
    """
    Creates one sub-task per requirement.
    Best when requirements are independent.
    """
    sub_tasks = []
    for req_id in task_proposal.requirements:
        requirement = load_requirement(req_id)
        sub_task = create_sub_task(
            title=f"{task_proposal.title} - {requirement.title}",
            description=requirement.description,
            requirements=[req_id],
            parent=task_proposal.id
        )
        sub_tasks.append(sub_task)
    return sub_tasks
```

**Example**:
```
Original: "Implement user authentication system" (complexity 8)
  Requirements: REQ-001 (Login), REQ-002 (Logout), REQ-003 (Session)

Breakdown:
  ‚Üí "Implement login functionality" (complexity 5) - REQ-001
  ‚Üí "Implement logout functionality" (complexity 3) - REQ-002
  ‚Üí "Implement session management" (complexity 5) - REQ-003
```

---

### Algorithm 2: Break Down by File Changes
**When to Use**: High file count drives complexity

**Logic**:
```python
def break_down_by_files(task_proposal):
    """
    Groups related files into sub-tasks.
    Best when task modifies many files.
    """
    file_groups = group_related_files(task_proposal.files_to_modify)
    sub_tasks = []
    for group_name, files in file_groups.items():
        sub_task = create_sub_task(
            title=f"{task_proposal.title} - {group_name}",
            description=f"Modify {len(files)} files in {group_name}",
            files=files,
            parent=task_proposal.id
        )
        sub_tasks.append(sub_task)
    return sub_tasks
```

**Example**:
```
Original: "Refactor authentication module" (complexity 8)
  Files: 12 files across services, controllers, models

Breakdown:
  ‚Üí "Refactor authentication services" (complexity 4) - 4 files
  ‚Üí "Refactor authentication controllers" (complexity 5) - 5 files
  ‚Üí "Refactor authentication models" (complexity 3) - 3 files
```

---

### Algorithm 3: Break Down by Risk Mitigation
**When to Use**: High risk factors drive complexity

**Logic**:
```python
def break_down_by_risk_mitigation(task_proposal):
    """
    Separates planning, implementation, and validation.
    Best for security, schema changes, breaking changes.
    """
    sub_tasks = [
        create_sub_task(
            title=f"Plan {task_proposal.title}",
            description="Design approach and identify risks",
            phase="planning",
            parent=task_proposal.id
        ),
        create_sub_task(
            title=f"Implement {task_proposal.title}",
            description="Core implementation with risk mitigations",
            phase="implementation",
            parent=task_proposal.id,
            depends_on=[planning_task.id]
        ),
        create_sub_task(
            title=f"Validate {task_proposal.title}",
            description="Security audit and integration testing",
            phase="validation",
            parent=task_proposal.id,
            depends_on=[implementation_task.id]
        )
    ]
    return sub_tasks
```

**Example**:
```
Original: "Add OAuth2 authentication" (complexity 9)
  Risk: Security, external dependencies, breaking changes

Breakdown:
  ‚Üí "Plan OAuth2 integration" (complexity 4) - Design phase
  ‚Üí "Implement OAuth2 provider" (complexity 6) - Implementation phase
  ‚Üí "Security audit OAuth2 flow" (complexity 4) - Validation phase
```

---

### Algorithm 4: Break Down by BDD Scenarios
**When to Use**: Multiple BDD scenarios in one task

**Logic**:
```python
def break_down_by_scenarios(task_proposal):
    """
    Creates one sub-task per BDD scenario.
    Best for user-facing features with multiple flows.
    """
    sub_tasks = []
    for scenario_id in task_proposal.bdd_scenarios:
        scenario = load_bdd_scenario(scenario_id)
        sub_task = create_sub_task(
            title=f"{task_proposal.title} - {scenario.title}",
            description=scenario.description,
            bdd_scenarios=[scenario_id],
            parent=task_proposal.id
        )
        sub_tasks.append(sub_task)
    return sub_tasks
```

**Example**:
```
Original: "Implement password reset flow" (complexity 8)
  Scenarios: BDD-001 (Request reset), BDD-002 (Validate token), BDD-003 (Update password)

Breakdown:
  ‚Üí "Implement password reset request" (complexity 4) - BDD-001
  ‚Üí "Implement reset token validation" (complexity 5) - BDD-002
  ‚Üí "Implement password update" (complexity 4) - BDD-003
```

---

### Algorithm Selection Logic
```python
def select_breakdown_algorithm(task_proposal, complexity_score):
    """
    Chooses the best breakdown algorithm based on complexity factors.
    """
    factors = complexity_score.factors

    # Priority order:
    if factors.risk_level >= 3:
        return break_down_by_risk_mitigation

    elif factors.file_complexity >= 3:
        return break_down_by_files

    elif len(task_proposal.requirements) > 3:
        return break_down_by_requirements

    elif len(task_proposal.bdd_scenarios) > 2:
        return break_down_by_scenarios

    else:
        # Default: break by implementation phases
        return break_down_by_phases

def break_down_by_phases(task_proposal):
    """
    Default breakdown: design ‚Üí implement ‚Üí test.
    """
    return [
        create_sub_task("Design " + task_proposal.title, phase="design"),
        create_sub_task("Implement " + task_proposal.title, phase="implementation"),
        create_sub_task("Test " + task_proposal.title, phase="testing")
    ]
```

---

## 6. Output Formatting Requirements

### Console Output Format
```
üîÑ Generating Tasks for FEAT-001: User Authentication

üìã Feature Analysis
Title: User Authentication
Epic: EPIC-001 (User Management System)
Requirements: 3 linked (REQ-001, REQ-002, REQ-003)
BDD Scenarios: 2 linked (BDD-001, BDD-002)
Acceptance Criteria: 4 defined

üéØ Task Generation Strategy
Based on analysis, generating these task types:
‚úÖ UI Tasks: 2 tasks
‚úÖ API Tasks: 3 tasks
‚úÖ Testing Tasks: 4 tasks
‚úÖ Documentation: 1 task

‚öôÔ∏è  Complexity Evaluation
Evaluating complexity for all proposed tasks...
Threshold: 7/10 (tasks >= 7 will be broken down)

üìã Generated Tasks (12)

üé® UI/UX Tasks
‚úÖ TASK-001.1.01: Design authentication UI components
   Complexity: 4/10 üü° Medium
   Factors: 3 files, 1 pattern (MVVM), low risk
   Timeline: 2 days | Estimated LOC: 150

‚úÖ TASK-001.1.02: Implement responsive login form
   Complexity: 3/10 üü¢ Simple
   Factors: 2 files, standard patterns, low risk
   Timeline: 1 day | Estimated LOC: 80

üîå API/Backend Tasks
‚úÖ TASK-001.1.03: Implement authentication API endpoint
   Complexity: 6/10 üü° Medium
   Factors: 4 files, 2 patterns (Repository, Either), medium risk
   Timeline: 2 days | Estimated LOC: 200

‚ö†Ô∏è  TASK-001.1.04: Add session management system
   Complexity: 8/10 üî¥ Complex ‚Üí Auto-breaking down...

   üîß Breakdown Strategy: Risk Mitigation (security concerns)
   Creating 3 sub-tasks:

   ‚úÖ TASK-001.1.04.1: Plan session storage architecture
      Complexity: 4/10 üü° Medium
      Factors: Design phase, security planning, 2 files
      Timeline: 1 day | Estimated LOC: 50 (documentation)

   ‚úÖ TASK-001.1.04.2: Implement session CRUD operations
      Complexity: 5/10 üü° Medium
      Factors: 3 files, standard patterns, medium risk
      Timeline: 1 day | Estimated LOC: 120
      Depends on: TASK-001.1.04.1

   ‚úÖ TASK-001.1.04.3: Add session security and expiration
      Complexity: 6/10 üü° Medium
      Factors: 2 files, security patterns, high risk mitigation
      Timeline: 1 day | Estimated LOC: 100
      Depends on: TASK-001.1.04.2

üß™ Testing Tasks (Auto-generated)
‚úÖ TASK-001.1.05: Create authentication unit tests
   Complexity: 4/10 üü° Medium
   Factors: 5 test files, xUnit patterns, low risk
   Timeline: 1 day | Estimated LOC: 300 (test code)
   Tests: TASK-001.1.01, TASK-001.1.02, TASK-001.1.03

‚úÖ TASK-001.1.06: Implement authentication integration tests
   Complexity: 5/10 üü° Medium
   Factors: 3 test files, API testing, medium risk
   Timeline: 1 day | Estimated LOC: 250
   Tests: TASK-001.1.03, TASK-001.1.04.x

‚úÖ TASK-001.1.07: Create E2E authentication tests
   Complexity: 6/10 üü° Medium
   Factors: 2 test files, Playwright, user flow complexity
   Timeline: 2 days | Estimated LOC: 200
   Tests: Complete user authentication flow

üìù Documentation Tasks
‚úÖ TASK-001.1.08: Create authentication API documentation
   Complexity: 2/10 üü¢ Simple
   Factors: 1 file, standard documentation, low risk
   Timeline: 0.5 days | Estimated LOC: 100 (markdown)

üìä Generation Summary
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total Tasks Created: 12 (1 broken down into 3 sub-tasks)
Original Task Count: 10
Breakdown Operations: 1 (TASK-001.1.04)

Complexity Distribution:
  Simple (1-3):  2 tasks (17%) üü¢
  Medium (4-6):  9 tasks (75%) üü°
  Complex (7-10): 0 tasks (0%)  üî¥ [All broken down]

Average Complexity: 4.3/10 (Medium)
Highest Complexity: 6/10 (TASK-001.1.04.3)

Estimated Timeline: 13.5 days
Estimated Total LOC: 1,550 lines

üîó Feature Integration
All tasks linked to FEAT-001
Epic context: EPIC-001
External tool inheritance: Jira (PROJECT-123), Linear (INIT-456)

‚úÖ Quality Checks
‚úì All tasks have testable acceptance criteria
‚úì All tasks linked to requirements or BDD scenarios
‚úì Testing tasks cover all implementation tasks
‚úì No tasks with complexity > 7 created
‚úì No duplicate tasks detected

üìÅ File Updates
Updated: docs/features/FEAT-001-user-authentication.md
Task files created in: tasks/backlog/
State tracking: docs/state/FEAT-001/task-generation-log.json

üìã Next Steps:
1. Review generated tasks: /feature-status FEAT-001 --tasks
2. Begin implementation: /task-work TASK-001.1.01
3. Export to PM tools: /feature-sync FEAT-001 --include-tasks
4. Track progress: /feature-status FEAT-001 --progress
```

---

## 7. Testing Requirements

### Unit Tests Required

#### Test Suite 1: Complexity Evaluation
**File**: `tests/test_feature_generate_tasks_complexity.py`

```python
def test_complexity_evaluation_applied_to_all_tasks():
    """AC-1: All tasks must have complexity scores."""
    feature = create_test_feature(task_count=5)
    result = run_feature_generate_tasks(feature)

    for task in result.tasks:
        assert task.complexity_score is not None
        assert 1 <= task.complexity_score <= 10
        assert task.complexity_level in ['simple', 'medium', 'complex']

def test_complexity_consistency_with_task_create():
    """AC-10: Same task description must have same complexity."""
    task_description = "Implement user authentication API"

    # Evaluate via /task-create
    create_score = evaluate_via_task_create(task_description)

    # Evaluate via /feature-generate-tasks
    generate_score = evaluate_via_feature_generate(task_description)

    assert create_score.total_score == generate_score.total_score
    assert create_score.level == generate_score.level

def test_complexity_factors_extraction():
    """Verify complexity factors are extracted correctly."""
    task_proposal = {
        "title": "Refactor authentication module",
        "files_to_modify": ["service.py", "controller.py", "model.py", "tests.py"],
        "patterns": ["Repository", "Either"],
        "dependencies": ["bcrypt", "jwt"],
        "risks": ["security", "breaking change"]
    }

    score = evaluate_complexity(task_proposal)

    assert score.factors.file_complexity >= 1.5  # 4 files
    assert score.factors.pattern_familiarity >= 1.0  # 2 patterns
    assert score.factors.risk_level >= 2.0  # Security + breaking
```

#### Test Suite 2: Task Breakdown
**File**: `tests/test_task_breakdown_algorithms.py`

```python
def test_automatic_breakdown_for_complex_task():
    """AC-2: Tasks with complexity >= 7 must be broken down."""
    task_proposal = create_complex_task_proposal(complexity=8)

    result = run_feature_generate_tasks_with_task(task_proposal)

    # Original complex task should not be created
    assert not task_file_exists(task_proposal.id)

    # Sub-tasks should exist
    sub_tasks = result.sub_tasks_for(task_proposal.id)
    assert len(sub_tasks) >= 2

    # All sub-tasks must have complexity <= 6
    for sub_task in sub_tasks:
        assert sub_task.complexity_score <= 6

def test_no_tasks_with_complexity_over_7_created():
    """AC-9: No task files with complexity > 7 exist after generation."""
    feature = create_test_feature(complex_tasks=True)
    run_feature_generate_tasks(feature)

    # Check all task directories
    for directory in ['backlog', 'in_progress', 'blocked', 'in_review']:
        tasks = load_all_tasks_from(directory)
        for task in tasks:
            assert task.complexity_score <= 7, \
                f"Task {task.id} has complexity {task.complexity_score} > 7"

def test_breakdown_by_requirements():
    """Test requirement-based breakdown algorithm."""
    task_proposal = {
        "title": "Implement authentication",
        "requirements": ["REQ-001", "REQ-002", "REQ-003"],
        "complexity": 9
    }

    sub_tasks = break_down_by_requirements(task_proposal)

    assert len(sub_tasks) == 3
    assert sub_tasks[0].requirements == ["REQ-001"]
    assert sub_tasks[1].requirements == ["REQ-002"]
    assert sub_tasks[2].requirements == ["REQ-003"]

def test_breakdown_by_files():
    """Test file-based breakdown algorithm."""
    task_proposal = {
        "title": "Refactor module",
        "files_to_modify": [
            "services/auth_service.py",
            "services/session_service.py",
            "controllers/auth_controller.py",
            "controllers/user_controller.py",
            "models/user.py",
            "models/session.py"
        ],
        "complexity": 8
    }

    sub_tasks = break_down_by_files(task_proposal)

    assert len(sub_tasks) == 3  # Services, controllers, models
    assert all(st.complexity_score <= 6 for st in sub_tasks)

def test_recursive_breakdown():
    """Test that breakdown is recursive for sub-tasks with complexity >= 7."""
    task_proposal = create_extremely_complex_task(complexity=10)

    result = run_breakdown(task_proposal)

    # Even after initial breakdown, some sub-tasks might be complex
    # Verify they are broken down further
    all_final_tasks = result.get_all_leaf_tasks()
    for task in all_final_tasks:
        assert task.complexity_score <= 6
```

#### Test Suite 3: Duplicate Detection
**File**: `tests/test_duplicate_detection.py`

```python
def test_exact_duplicate_detection():
    """AC-6: Detect exact duplicate task titles."""
    create_task("Implement user authentication", feature="FEAT-001")

    proposal = {"title": "Implement user authentication", "feature": "FEAT-001"}
    result = check_for_duplicates(proposal)

    assert result.is_duplicate
    assert result.similarity_score >= 1.0

def test_fuzzy_duplicate_detection():
    """Detect similar task titles with typos/variations."""
    create_task("Implement user authentication", feature="FEAT-001")

    proposals = [
        "Implement user authentification",  # Typo
        "Add user authentication",  # Different verb
        "User authentication implementation"  # Different structure
    ]

    for proposal_title in proposals:
        result = check_for_duplicates({"title": proposal_title, "feature": "FEAT-001"})
        assert result.is_duplicate or result.similarity_score >= 0.75

def test_no_false_positives():
    """Verify unrelated tasks are not flagged as duplicates."""
    create_task("Implement user authentication", feature="FEAT-001")

    unrelated = [
        "Implement user profile page",
        "Add unit tests for authentication",
        "Create password reset flow"
    ]

    for proposal_title in unrelated:
        result = check_for_duplicates({"title": proposal_title, "feature": "FEAT-001"})
        assert not result.is_duplicate
        assert result.similarity_score < 0.75

def test_duplicate_detection_across_directories():
    """Verify duplicates are detected in all task states."""
    # Create task in different directories
    create_task_in_backlog("Implement authentication", feature="FEAT-001")
    create_task_in_in_progress("Implement authentication", feature="FEAT-001")

    proposal = {"title": "Implement authentication", "feature": "FEAT-001"}
    result = check_for_duplicates(proposal)

    assert result.is_duplicate
    assert len(result.similar_tasks) >= 2  # Found in multiple directories
```

#### Test Suite 4: Interactive Mode
**File**: `tests/test_interactive_complexity_adjustment.py`

```python
def test_interactive_threshold_customization():
    """AC-4: Users can customize complexity threshold in interactive mode."""
    feature = create_test_feature()

    # Mock user input: threshold = 6
    with mock_user_input(["6", "y"]):
        result = run_feature_generate_tasks(feature, interactive=True)

    assert result.complexity_threshold == 6

    # Tasks with complexity >= 6 should be broken down
    for task in result.tasks:
        assert task.complexity_score < 6

def test_interactive_breakdown_override():
    """Test user can override automatic breakdown decisions."""
    complex_task = create_task_proposal(complexity=8)

    # Mock user chooses to keep complex task (override)
    with mock_user_input(["K"]):  # Keep as-is
        result = run_interactive_breakdown_prompt(complex_task)

    assert result.action == "keep"
    assert result.override_reason is not None

def test_interactive_view_complexity_factors():
    """Test user can view detailed complexity factors."""
    complex_task = create_task_proposal(complexity=7)

    with mock_user_input(["V", "A"]):  # View, then Approve
        result = run_interactive_breakdown_prompt(complex_task)

    assert result.viewed_factors
    assert result.action == "approve"
```

#### Test Suite 5: Quality Gate Integration
**File**: `tests/test_quality_gate_integration.py`

```python
def test_testing_tasks_auto_generated():
    """AC-5: Testing tasks are auto-generated for complex features."""
    feature = create_complex_feature(implementation_task_count=6)

    result = run_feature_generate_tasks(feature)

    # Find testing tasks
    testing_tasks = [t for t in result.tasks if "test" in t.title.lower()]

    assert len(testing_tasks) >= 3  # Unit, integration, E2E
    assert any("unit" in t.title.lower() for t in testing_tasks)
    assert any("integration" in t.title.lower() for t in testing_tasks)
    assert any("e2e" in t.title.lower() for t in testing_tasks)

def test_all_tasks_have_acceptance_criteria():
    """Verify all tasks have testable acceptance criteria."""
    feature = create_test_feature(task_count=5)
    result = run_feature_generate_tasks(feature)

    for task in result.tasks:
        assert len(task.acceptance_criteria) >= 1
        assert all(is_testable(criterion) for criterion in task.acceptance_criteria)

def test_tasks_linked_to_requirements():
    """Verify tasks are linked to requirements or BDD scenarios."""
    feature = create_feature_with_requirements()
    result = run_feature_generate_tasks(feature)

    for task in result.tasks:
        has_requirements = len(task.requirements) > 0
        has_bdd = len(task.bdd_scenarios) > 0
        assert has_requirements or has_bdd, \
            f"Task {task.id} has no requirements or BDD scenarios"
```

#### Test Suite 6: Output Formatting
**File**: `tests/test_output_formatting.py`

```python
def test_complexity_visualization_in_output():
    """AC-3: Complexity scores and visual indicators are displayed."""
    feature = create_test_feature(task_count=5)

    output = capture_output(lambda: run_feature_generate_tasks(feature))

    # Check for complexity scores
    assert re.search(r'Complexity: \d+/10', output)

    # Check for emoji indicators
    assert 'üü¢' in output or 'üü°' in output or 'üî¥' in output

    # Check for distribution statistics
    assert 'Average Complexity:' in output
    assert 'Complexity Distribution:' in output

def test_breakdown_rationale_displayed():
    """Verify breakdown decisions include explanations."""
    complex_task = create_task_proposal(complexity=8)

    output = capture_output(lambda: run_feature_generate_tasks_with_task(complex_task))

    assert 'Auto-breaking down' in output
    assert 'Breakdown Strategy:' in output
    assert 'Creating' in output and 'sub-tasks' in output
```

### Integration Tests Required

```python
def test_end_to_end_task_generation_with_complexity():
    """Complete workflow from feature to task files."""
    # Create feature with requirements and BDD scenarios
    feature = create_feature(
        title="User Authentication",
        requirements=["REQ-001", "REQ-002", "REQ-003"],
        bdd_scenarios=["BDD-001", "BDD-002"],
        acceptance_criteria=[...],
        epic="EPIC-001"
    )

    # Run task generation
    run_feature_generate_tasks(feature.id)

    # Verify task files created
    tasks = load_tasks_for_feature(feature.id)
    assert len(tasks) >= 5

    # Verify all tasks have complexity metadata
    for task in tasks:
        assert task.frontmatter['complexity']['score'] is not None
        assert task.frontmatter['complexity']['level'] in ['simple', 'medium', 'complex']

    # Verify no complex tasks exist
    for task in tasks:
        assert task.frontmatter['complexity']['score'] <= 7

    # Verify testing tasks exist
    testing_tasks = [t for t in tasks if 'test' in t.title.lower()]
    assert len(testing_tasks) >= 2

def test_integration_with_task_create_consistency():
    """Verify complexity evaluation is consistent across commands."""
    task_description = "Implement OAuth2 authentication provider"

    # Create via /task-create
    task_create_result = run_task_create(task_description, epic="EPIC-001")

    # Generate via /feature-generate-tasks
    feature = create_feature_with_task_proposal(task_description)
    feature_generate_result = run_feature_generate_tasks(feature.id)

    # Compare complexity scores
    create_task = load_task(task_create_result.task_id)
    generated_task = feature_generate_result.tasks[0]

    assert create_task.complexity_score == generated_task.complexity_score
```

### Test Coverage Targets
- **Unit test coverage**: ‚â• 80% line coverage
- **Branch coverage**: ‚â• 75% for breakdown algorithms
- **Integration test coverage**: All critical workflows (AC-1 through AC-10)
- **Edge case coverage**: 100% of identified edge cases

---

## 8. Identified Gaps and Questions

### Gap 1: Complexity Evaluation Module Location
**Status**: UNKNOWN
**Question**: Where is the complexity evaluation module from TASK-005?
**Expected Path**: `installer/global/commands/lib/complexity_evaluator.py`
**Impact**: BLOCKING - Cannot proceed without this module
**Action Required**: Locate module or implement from scratch

### Gap 2: TASK-006 Integration Details
**Status**: UNKNOWN
**Question**: What did TASK-006 implement in `/task-create`?
**Impact**: HIGH - Affects consistency requirements (AC-10)
**Action Required**: Review TASK-006 completion report

### Gap 3: Breakdown Algorithm Selection
**Status**: UNCLEAR
**Question**: Should users be able to choose breakdown strategy, or should it be automatic?
**Impact**: MEDIUM - Affects interactive mode design
**Options**:
  - Option A: Fully automatic based on complexity factors
  - Option B: Suggest strategy, allow user to override
  - Option C: Always prompt in interactive mode
**Recommendation**: Option B (suggest with override)

### Gap 4: Minimum Task Size
**Status**: NOT SPECIFIED
**Question**: Is there a minimum complexity or scope for tasks?
**Impact**: MEDIUM - Prevents over-decomposition
**Recommendation**: Set minimum complexity = 2 (tasks scoring 1 are likely too trivial)

### Gap 5: Recursion Depth Limit
**Status**: NOT SPECIFIED
**Question**: How deep should recursive breakdown go?
**Impact**: LOW - Prevents infinite loops
**Recommendation**: Maximum depth = 3 (Task ‚Üí Sub-task ‚Üí Sub-sub-task)

### Gap 6: Testing Task Complexity
**Status**: UNCLEAR
**Question**: Should testing tasks have fixed complexity or be evaluated?
**Impact**: LOW - Affects generation logic
**Options**:
  - Option A: Fixed complexity = 3 (simple)
  - Option B: Evaluate based on test count and coverage
**Recommendation**: Option A for MVP, Option B for future enhancement

### Gap 7: Duplicate Detection Threshold
**Status**: NOT SPECIFIED
**Question**: What similarity score triggers duplicate warning?
**Impact**: MEDIUM - Affects user experience
**Options**: 75%, 80%, 85%
**Recommendation**: 80% (balance between catching duplicates and false positives)

### Gap 8: Plain Text Fallback
**Status**: NOT SPECIFIED
**Question**: How should output render in terminals without emoji support?
**Impact**: LOW - Affects accessibility
**Recommendation**: Fallback to text indicators: [S], [M], [C] for Simple, Medium, Complex

### Gap 9: Complexity Metadata Persistence
**Status**: UNCLEAR
**Question**: Should complexity evaluations be logged for analysis?
**Impact**: LOW - Affects future improvements
**Recommendation**: Yes, save to `docs/state/FEAT-XXX/complexity-log.json`

### Gap 10: Breaking Changes vs New Features
**Status**: UNCLEAR
**Question**: Do we need different breakdown strategies for breaking changes vs new features?
**Impact**: MEDIUM - Affects algorithm selection
**Recommendation**: Yes, breaking changes should use risk mitigation breakdown (planning ‚Üí implementation ‚Üí validation)

---

## 9. Implementation Recommendations

### Phase 1: Foundation (Days 1-2)
**Priority**: CRITICAL

1. **Locate/Create Complexity Evaluator Module**
   - Find TASK-005 implementation
   - If not found, implement from scratch
   - Unit test with known task descriptions
   - Document API contract

2. **Review TASK-006 Integration**
   - Review `/task-create` complexity integration
   - Identify reusable components
   - Document consistency requirements

3. **Design Task Breakdown Algorithms**
   - Implement 4 core algorithms (requirements, files, risk, scenarios)
   - Add algorithm selection logic
   - Unit test each algorithm with edge cases

### Phase 2: Core Implementation (Days 3-4)
**Priority**: CRITICAL

1. **Integrate Complexity Evaluation**
   - Add complexity evaluation to task proposal generation
   - Implement automatic breakdown for complexity ‚â• 7
   - Add complexity metadata to task frontmatter

2. **Implement Duplicate Detection**
   - Add fuzzy string matching
   - Search across all task directories
   - Implement warning prompts

3. **Update Output Formatting**
   - Add complexity scores to console output
   - Add visual indicators (emoji + plain text fallback)
   - Add distribution statistics

### Phase 3: Interactive Mode (Days 5-6)
**Priority**: HIGH

1. **Add Complexity Threshold Customization**
   - Prompt for threshold in interactive mode
   - Re-evaluate with custom threshold
   - Allow override decisions

2. **Add Breakdown Review Prompts**
   - Show breakdown rationale
   - Allow manual adjustment
   - View detailed complexity factors

### Phase 4: Quality & Testing (Days 7-8)
**Priority**: HIGH

1. **Comprehensive Testing**
   - Unit tests for all algorithms
   - Integration tests for end-to-end workflow
   - Test consistency with `/task-create`

2. **Documentation Updates**
   - Update `feature-generate-tasks.md`
   - Add complexity control examples
   - Document breakdown strategies

### Phase 5: Polish & Edge Cases (Day 9)
**Priority**: MEDIUM

1. **Handle Edge Cases**
   - Recursion depth limits
   - Minimum task size
   - Testing task generation

2. **Performance Optimization**
   - Optimize duplicate detection
   - Cache complexity evaluations
   - Parallel evaluation for large features

---

## 10. Success Metrics

### Quantitative Metrics
- **100%** of generated tasks have complexity ‚â§ 7
- **0** tasks created with missing complexity metadata
- **‚â• 90%** duplicate detection accuracy (true positives / total duplicates)
- **< 5%** false positive rate for duplicate detection
- **‚â• 80%** unit test coverage
- **< 30 seconds** generation time for features with 20 tasks
- **100%** consistency with `/task-create` complexity scores

### Qualitative Metrics
- **Clear rationale** for all breakdown decisions
- **Actionable feedback** in interactive mode
- **User-friendly** complexity visualization
- **Comprehensive** documentation with examples
- **Consistent** with existing command patterns

---

## 11. Risk Assessment

### High Risk
1. **Complexity Module Not Found**
   - **Mitigation**: Implement from scratch, reference TASK-005 requirements
   - **Contingency**: 2-day buffer for implementation

2. **Inconsistency with /task-create**
   - **Mitigation**: Share evaluation module, add consistency tests
   - **Contingency**: Refactor both commands to use shared library

3. **Breakdown Algorithm Complexity**
   - **Mitigation**: Start with simple algorithms, iterate based on usage
   - **Contingency**: Manual breakdown as fallback

### Medium Risk
1. **Performance Issues with Large Features**
   - **Mitigation**: Optimize early, use caching
   - **Contingency**: Add progress indicators, async evaluation

2. **User Confusion with Interactive Mode**
   - **Mitigation**: Clear prompts, helpful defaults
   - **Contingency**: Simplify interactive flow, focus on auto mode

3. **Duplicate Detection False Positives**
   - **Mitigation**: Tune similarity threshold, add user confirmation
   - **Contingency**: Allow user to force creation

### Low Risk
1. **Emoji Rendering Issues**
   - **Mitigation**: Plain text fallback
   - **Contingency**: Configuration flag to disable emoji

2. **Edge Case Handling**
   - **Mitigation**: Comprehensive testing
   - **Contingency**: Add validation and error messages

---

## 12. Next Steps

### Immediate Actions
1. **Locate TASK-005 complexity module** or confirm need to implement from scratch
2. **Review TASK-006 completion** to understand `/task-create` integration
3. **Clarify outstanding questions** (Gaps 3-10) with stakeholders
4. **Begin Phase 1 implementation** (Foundation)

### Blocked Dependencies
- TASK-005 complexity module location
- TASK-006 integration details

### Recommended Timeline
- **Phase 1-2**: 4 days (Foundation + Core)
- **Phase 3**: 2 days (Interactive)
- **Phase 4**: 2 days (Testing)
- **Phase 5**: 1 day (Polish)
- **Total**: 9 days

---

## Appendix A: EARS Requirements (Formalized)

### REQ-008.1: Complexity Evaluation
**Type**: Event-Driven
**EARS**: When `/feature-generate-tasks` generates task proposals, the system shall evaluate each task's complexity using the complexity evaluation module from TASK-005.

### REQ-008.2: Automatic Breakdown
**Type**: Unwanted Behavior
**EARS**: If a proposed task has complexity score ‚â• 7, then the system shall automatically break it down into sub-tasks with complexity ‚â§ 6.

### REQ-008.3: Complexity Visualization
**Type**: Ubiquitous
**EARS**: The system shall display complexity scores with visual indicators (üü¢ üü° üî¥) for all generated tasks.

### REQ-008.4: Interactive Threshold
**Type**: Optional Feature
**EARS**: Where `--interactive` mode is enabled, the system shall allow users to customize the complexity threshold for automatic breakdown.

### REQ-008.5: Testing Task Generation
**Type**: State-Driven
**EARS**: While generating tasks for features with high complexity, the system shall automatically generate unit, integration, and E2E testing tasks.

### REQ-008.6: Duplicate Prevention
**Type**: Ubiquitous
**EARS**: The system shall validate that no duplicate tasks exist across all task directories before creating new tasks.

### REQ-008.7: Metadata Persistence
**Type**: Ubiquitous
**EARS**: The system shall include complexity metadata (score, level, factors) in task frontmatter for all generated tasks.

### REQ-008.8: Consistency Enforcement
**Type**: Ubiquitous
**EARS**: The system shall use the same complexity evaluation algorithm as `/task-create` to ensure consistent scoring.

### REQ-008.9: No Complex Tasks Created
**Type**: Unwanted Behavior
**EARS**: If task generation completes, then no tasks with complexity > 7 shall exist in any task directory.

### REQ-008.10: Distribution Statistics
**Type**: Ubiquitous
**EARS**: The system shall display complexity distribution statistics (average, count by level) in the generation summary.

---

## Appendix B: File Paths Reference

```
# Existing files (expected)
installer/global/commands/feature-generate-tasks.md
installer/global/commands/task-create.md
installer/global/commands/task-work.md
installer/global/commands/lib/complexity_evaluator.py  # From TASK-005 (location TBD)
installer/global/commands/lib/task_breakdown.py  # New (to be created)

# New files (to be created)
installer/global/commands/lib/duplicate_detector.py
installer/global/commands/lib/output_formatter.py
tests/test_feature_generate_tasks_complexity.py
tests/test_task_breakdown_algorithms.py
tests/test_duplicate_detection.py
tests/test_interactive_complexity_adjustment.py
tests/test_quality_gate_integration.py
tests/test_output_formatting.py

# Documentation updates
installer/global/commands/feature-generate-tasks.md  # Update with complexity controls
docs/guides/FEATURE-GENERATE-TASKS-COMPLEXITY-GUIDE.md  # New guide
```

---

**End of Requirements Analysis**
